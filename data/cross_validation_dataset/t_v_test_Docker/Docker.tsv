GG	Hi guys. __eou__	user	0	1	1	2
OQ	i have stupd question;i have two physical server with docker engine on board.1 - with mysql container2 - with apps containers __eou__	user	0	1	0	0
FD	I need to create a network between the two docker-host __eou__	user	0	1	0	0
FD	what should I do? what tool to use? __eou__	user	0	1	0	0
IG	gudron: , have you checked this [<-LINK->] ? __eou__	agent	1	1	0	1
GG	hm... __eou__	user	0	1	0	0
FQ	i must do it on every host? __eou__	user	0	1	0	0

OQ	How I can start nginx and php on starting container? __eou__	user	0	1	1	2
FD	 [<-CODE->] not working (( __eou__	user	0	1	0	0
IG	krzysztof-magosa: @sirrolandif you want/bin/sh to be your interpreter just supply CMD as a quoted array as you have; see: [<-LINK->]  __eou__	agent	1	1	0	1
IG	sirroland: you'll probably need to put those two commands in a single  .sh file and execute it via cmd __eou__	agent	1	1	0	1
AE	@sirroland I think you simply need && after the first start so [<-CODE->] CMD executes a single command so by using && you get it to do both - at least thats my understanding __eou__	agent	1	1	0	1
FD	although ideally as has already been mentioned a shell script would be a good idea __eou__	agent	0	1	0	0

OP	hi all I\'m trying start nginx via:docker run --network "host" -e NGINX_PORT=8080 2f3c6710d8f2but I get only:2016/11/25 12:29:06 [emerg] 1#1: bind() to 0.0.0.0:80 failed (98: Address in use)\nnginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address in use) __eou__	user	0	1	1	2
FD	How I can change the port in [<-LINK->] image ...? :/ __eou__	user	0	1	0	0
IG	you should use the --expose option for this __eou__	agent	0	1	0	0
FD	 [<-CODE->]  __eou__	agent	0	1	0	0
FD	have a look at the commandline  reference here [<-LINK->] or directly in the nginx image docs [<-LINK->] in the section exposing ports, there you have a direct example __eou__	agent	0	1	0	0
FD	 [<-CODE->] this would map your local port 8080 to the port 80 in the container __eou__	agent	0	1	0	0
NF	lucasjahn: still have same problem __eou__	user	0	1	0	0
CC	withdocker run --network "host" -p 8080:80 -e NGINX_PORT=8080 2f3c6710d8f2 __eou__	user	0	1	0	0
IG	you set NGINX_PORT=8080 __eou__	agent	0	1	0	0
FD	so you should adjust __eou__	agent	0	1	0	0
FD	-p 8080:8080 __eou__	agent	0	1	0	0
FD	to this __eou__	agent	0	1	0	0
CQ	can you just paste in your error again in here? __eou__	agent	0	1	0	0
FF	docker run --network "host" -p 8080:8080 -e NGINX_PORT=8080 2f3c6710d8f2 __eou__	user	0	1	0	0
NF	still same error __eou__	user	0	1	0	0
FD	 [<-CODE->]  __eou__	user	0	1	0	0
FD	it's ignore my 8080 settings :/ __eou__	user	0	1	0	0
IG	uhoreg Are you sure the image you\'re using supports NGINX_PORT?  Don\'t use \'--network "host"\'.  Just do "docker run -p 8080:80 nginx" __eou__	agent	1	1	0	1
CC	but I want --network "host" __eou__	user	0	1	0	0
FD	I'm using this image: [<-LINK->]  __eou__	user	0	1	0	0
GG	hmm... __eou__	user	0	1	0	0
FD	"Out-of-the-box, Nginx doesn\'t support using environment variables inside most configuration blocks. But envsubst..." __eou__	user	0	1	0	0
CC	and what happens if you just run it like@matrixbotsaid? __eou__	agent	0	1	0	0
FD	still the error? __eou__	agent	0	1	0	0
PF	lucasjahn: no.. it\'s ok.  __eou__	user	0	1	0	0
AC	but I need --network "host" option ... so I must build my own image from nginx:alpine ... __eou__	user	1	1	0	1

OQ	So I'm trying to figure out a way to build images that arecompletedbuilds for my rails app deployments. I've basically read a bunch of stuff and come to the conclusion that I should create an image that has all my base dependencies like ruby version, nodejs, etc. Then i should create a dockerfile using that image as base, then copying over my 'tmp' directory that contains build cache, my 'node_modules' dir that contains previously downloaded javascript dependencies, and similarly so for my gems, and my javascript and image/css precompiled assets. After copying them into the image i can run my ruby 'build' tasks, then save all the new tmp, node_modules etc contents back to the host computer filesystem, and finish off the dockerfile. __eou__	user	0	1	1	2
FD	If i'm not mistake this can give me very fast build times with minimal downloading etc and leads to a single image that I can spin up on any number of machines very quickly __eou__	user	0	1	0	0
FD	does this approach make any sense? __eou__	user	0	1	0	0
FD	apologies for the lack of sensible terminology. __eou__	user	0	1	0	0
IG	if you want to leverage caching of build tasks, yes, that's the best solution. But to release a container for production, I usually remove build tools and ship the container with the minimum to run the service. __eou__	agent	0	1	0	0
CC	I dont understand shipping containers __eou__	user	0	1	0	0
FD	i thought you ship images, and when you run them you end up with a container on the host system __eou__	user	0	1	0	0
IG	are you saying i should build everything first, then copy what i need into a docker image? __eou__	user	1	1	0	1
PF	that could work too __eou__	user	0	1	0	0

OP	can an application running inside a container detect which port was used as public in-p xxxx:YYYYY? i.e I want to detect what is the port outside the container so the APP can listen to it even when using random ports generated by docker __eou__	user	0	0	0	0
IG	docker inspect <container_id> __eou__	agent	0	0	0	0
FD	that'll give you everything docker knows about the container __eou__	agent	0	0	0	0
CC	killerspaz: I mean from inside the container __eou__	user	0	0	0	0
IG	did you see this? [<-ISSUE->] __eou__	agent	0	0	0	0
CC	I mean, after docker spin up the container, my app needs to be aware of its IP and port exposed by docker so it can start listening on that port __eou__	user	0	0	0	0
IG	I would pass those as environment variables if you can — should be easier and safer to run on different machines __eou__	agent	0	0	0	0
CC	yeah was going to ask why not supply the port? __eou__	agent	0	0	0	0
FD	Because I dont wnat to be supplying ports each I spin a new container __eou__	user	0	0	0	0
FD	I just want to run it and it resolve itself __eou__	user	0	0	0	0
FD	It is pretty common and useful for those mocroservices scenarios __eou__	user	0	0	0	0
AC	Let me see that issue __eou__	user	0	0	0	0
FD	the issue still open, 3 years... __eou__	user	0	0	0	0
FD	anyway __eou__	user	0	0	0	0
FD	killerspaz: the way docker works today as you are stating works pretty well if you are using simple/regular webapps __eou__	user	0	0	0	0
FD	if you have a complex distributed cluster you will end up with this problem __eou__	user	0	0	0	0
FD	ok look at this [<-LINK->] __eou__	user	0	0	0	0
IG	yeah don't do that __eou__	agent	0	0	0	0
FD	use docker network properly and you won't have this issue __eou__	agent	0	0	0	0
CC	the two containers running that has the SiloHost.exe,EXPOSE 2323on its docker file, and they write to a membership table store its port __eou__	user	0	0	0	0
NF	no no __eou__	user	0	0	0	0
FD	listen __eou__	user	0	0	0	0
FD	(read :P ) __eou__	user	0	0	0	0
FD	theOrleansClient.exewhich may be a container or not, read from the Membership table storage and know that both silos are running on 2323 so it try to connect to one of them and fail __eou__	user	0	0	0	0
FD	because the port exposed is 3333 and 4444 __eou__	user	0	0	0	0
FD	imo, you shouldn't be spinning up docker containers manually (unless purely testing that single container alone) nor exposing ports manually (again, unless testing)... docker network bridges take care of all that for you. __eou__	agent	0	0	0	0
FD	again, all 50 of my services run on the same port __eou__	agent	0	0	0	0
NF	no it doesn't __eou__	user	0	0	0	0
FD	that is the point __eou__	user	0	0	0	0
FD	the client connects to the silos by reading the membership table __eou__	user	0	0	0	0
FD	in that drawing, you have no context of any network __eou__	agent	0	0	0	0
CC	we don't need the context __eou__	user	0	0	0	0
FD	ignore that __eou__	user	0	0	0	0
NF	no i'm telling you that you DO __eou__	agent	0	0	0	0
FD	in order for the containers to talk to each other without you having to worry about port mapping __eou__	agent	0	0	0	0
FD	unless you're not detailing enough information, this is the basis of docker and microservices, and not out of the norm __eou__	agent	0	0	0	0
FD	assume that the containers are reacheable to each other __eou__	user	0	0	0	0
FD	i have 50 services per cluster, and about 12 clusters across the globe, and don't have any issues with having to track down ports __eou__	agent	0	0	0	0
FQ	the problem is that the 1. Server app need to know the port and IP it will bind to so it can write it to membership table. 2. the client read this data from that table so it WILL need IP and PORT each silo is running __eou__	user	0	0	0	0
FD	trust me __eou__	user	0	0	0	0
CQ	so when you spin up your deployment, you're manually runningdocker runlines? __eou__	agent	0	0	0	0
FD	the application NEED that __eou__	user	0	0	0	0
NF	no dude, docker has all that built in natively __eou__	agent	0	0	0	0
NF	:( __eou__	user	0	0	0	0
FD	you don't need to know IPs at all __eou__	agent	0	0	0	0
NF	you are not understanding :( __eou__	user	0	0	0	0
CQ	so maybe you can elaborate a bit? Because I'm still not seeing the issue other than your understanding of docker/compose __eou__	agent	0	0	0	0
CC	ok... let me try again __eou__	user	0	0	0	0
FD	regardless of how docker manage dns resolution and port binding __eou__	user	0	0	0	0
FD	the server application needs to know which IP:PORT it will listen to and write to the membership store tables __eou__	user	0	0	0	0
CC	got it? __eou__	user	0	0	0	0
FD	now __eou__	user	0	0	0	0
CC	i'm pretty certain I understand what you're trying to do __eou__	agent	0	0	0	0
CC	the client which will read from the membership table will need to be able to reach IP:PORT __eou__	user	0	0	0	0
FD	you're basically building a mesh __eou__	agent	0	0	0	0
FD	an explicit mesh __eou__	agent	0	0	0	0
CC	it is a Distributed Hash Table __eou__	user	0	0	0	0
IG	which i'm telling you is redundant because docker has that native __eou__	agent	0	0	0	0
CC	that manages membership of the cluster __eou__	user	0	0	0	0
FD	ok... __eou__	user	0	0	0	0
FD	so __eou__	user	0	0	0	0
CQ	answer this, are you ever runningdocker runoutside of running unit tests? __eou__	agent	0	0	0	0
FD	for the local tests I'm using docker run __eou__	user	0	0	0	0
FD	but I plan to learn and use compose to make it production soon __eou__	user	0	0	0	0
IG	i'd get that going immediately, you'll see it works all that out really quick __eou__	agent	0	0	0	0
NF	it wont __eou__	user	0	0	0	0
FD	the cluster althought they are running the same image, when each machin join the cluster it join the DHT which makes it handle specific part of the messages __eou__	user	0	0	0	0
FD	it is not like a web server that you can reach a service or DNS name that will be round robin balanced to any server __eou__	user	0	0	0	0
FD	the client (based on that membership table) will need to connect specifically for a given silo/server to do specific stuff __eou__	user	0	0	0	0

OQ	I need a way to external containers to say "hey docker, that container is unhealthy" and eventually ask docker to kill it __eou__	user	0	1	1	2
FD	regardless what docker or the container itself think __eou__	user	0	1	0	0
FD	this pattern is common in distributed computing actually... __eou__	user	0	1	0	0
IG	K8s, Rancher, Swarm __eou__	agent	0	1	0	0
FD	Take what you need they all check for that __eou__	agent	0	1	0	0
CC	you mean they check themselves, not the containers __eou__	user	0	1	0	0
IG	K8s and Rancher has healthchecks for their "services" and swarm simply checks that all containers have a healthy state. And as long as every container has a healty state all containers work as expected and everything from outside is L4 mesh magic :D __eou__	agent	1	1	0	1
AE	i think he's saying they check the container's health status based on the result of that HEALTHCHECK entry as likely listed bydocker inspect, and have an option to auto-kill the container should it be unhealthy __eou__	agent	0	1	0	0
FQ	There are 2 types of health check. Local, where if unhealthy, just suicide(the HEALTHCHECK is a good candidate for that) and the distributed consensus, which is made by external sources trying to reach that container and failing, so they agree on ask docker to kill it __eou__	user	0	1	0	0
FD	In orleans case the consensus is what I'm having problems to implement in docker due to those limitations __eou__	user	0	1	0	0
PF	The self check can be made with the HEALTHCHECK just fine I guess __eou__	user	0	1	0	0

GG	hi all __eou__	user	0	1	1	2
OQ	Is there a way to set folder ownership on mounted volume? __eou__	user	0	1	0	0
FD	Somehow, when I change permissions once, it’s kept on my local docker but not on AWS. __eou__	user	0	1	0	0
AE	ShurikAg: I have a really similar file permission issue. So upvote for any helpful tipp on this :) __eou__	agent	0	1	0	0
IG	I'd suggest checking this out, since running as root is definitely an anti-pattern that should be avoided, [<-LINK->] __eou__	agent	1	1	0	1

OQ	how to define links in docker-compose file if i want to deploy on the warm __eou__	user	0	1	1	2
FD	swarm __eou__	user	0	1	0	0
IG	you should not use links in this case. Address the services and may add network aliases __eou__	agent	0	1	0	0
CC	means how??i m not getting you..like i m having 3 service ..one is redis ,another one is neo4j and third one my nodejs app which has to run on 8081 __eou__	user	0	1	0	0
FD	? __eou__	user	0	1	0	0
CQ	oh wait you talk about compose v3? __eou__	agent	0	1	0	0
CC	yes i m talking about that __eou__	user	0	1	0	0
FD	i dont know how to write compose version 3.but i have to user version  3 so that i cn deploy a stack in docker 1.13 using compose nt bundle __eou__	user	0	1	0	0
IG	 [<-LINK->]  __eou__	agent	1	1	0	1
FD	should be a nice example __eou__	agent	1	1	0	1
UF	ok i will explore...thanx __eou__	user	0	1	0	0
FQ	SISheogorath: but tell me i m having one environment vairable in nodejs app i.e.neo4j Url and i have to mention it in that and i have to refer that neo4j.so without links how i mention that? __eou__	user	0	1	0	0
IG	set the url to the servicename of your neo4j service. if you name it "foo" use " [<-LINK->] " as url. docker swarm networking will do the rest __eou__	agent	0	1	0	0
FQ	is it necessary that  i have to define the network also? __eou__	user	0	1	0	0
PF	yes __eou__	agent	0	1	0	0

OQ	So when I run a docker instance, my entrypoint runs some bootstrapping commands to sync dependancies (specifically php'scomposer install).  These dependencies are written to disk owned by root:root. Is there a way to tell docker (specifically docker-compose) to write files as the user I'm running docker as? __eou__	user	0	1	1	2
FD	Looks like docker-composer supports auserkey in the yaml file. I'll try it out, but it would be nice if there was an agnostic way to let all developers run the same config. __eou__	user	0	1	0	0
IG	sbbowers__twitter: typically this is construct you create inside your container that picks up the user running the container's ID and chown all the files inside the container to that user during the build __eou__	agent	1	1	0	1
FD	 [<-LINK->]  __eou__	agent	1	1	0	1
FQ	dragon788: So the "standard approach" is to bake in "gosu" into all of your docker images by curling a binary off the net and then require your entrypoint to run the command through that, which gives you the option of passing an environment variable with the ID of your current user, all of which won\'t work with docker-compose. Do I understand that right? __eou__	user	0	1	0	0
IG	I think gosu is one way to do it, but really since it isn't recommended to run as root anyways, a lot of containers will chmod everything to a known user and run everything as that, the trick is whether that UID exists on your system or not __eou__	agent	0	1	0	0
FQ	So the easy way would to do auseradd -u {some arbitrary ID I pick} docker-useron the host and then pass that user ID to docker's -u? __eou__	user	0	1	0	0
FD	If I specify "docker-user" to docker, will it resolve the ID from the host, or does it do a useradd and create a new user? __eou__	user	0	1	0	0
IG	 [<-LINK->]  __eou__	agent	0	1	0	0
FD	this appears to have a workaround on an open bug that is somewhat related, [<-ISSUE->]  __eou__	agent	0	1	0	0
UF	Thanks for your help,@dragon788 __eou__	user	0	1	0	0
GG	np@sbbowers__twitterI just make it up as I go along :D __eou__	agent	0	1	0	0

GG	hey guys __eou__	user	0	0	0	0
OP	quick question... __eou__	user	0	0	0	0
OP	is there a way to docker monitor if a service inside the container is alive or not? __eou__	user	0	0	0	0
FD	I mean, the main ENTRYPOINT is alive, so from docker perspective, the container is running however, some internal crap happen on the app and I need docker to check with some sort of heartbeat the application (idk, a ping on a specific port or whatever) and if it does'nt reply, just kill the container... is it possible? __eou__	user	0	0	0	0
IG	need some health monitoring internal and external to the container; something internal to keep checking the status of what you need running/accessible, and something external to poll it once in a while to know to kill/restart it.... there's unfortunately so many ways to accomplish this, it just depends on what level of effort you want to put into it. __eou__	agent	0	0	0	0
FD	you could simplyexecaps | grep APPto see if it's running external from container on the host __eou__	agent	0	0	0	0
GG	killerspaz: hey! :) __eou__	user	0	0	0	0
CC	that is the poin __eou__	user	0	0	0	0
FD	point __eou__	user	0	0	0	0
FD	in Orleans we have a health monitoring service which all nodes of the cluster ping each other and after a given number of ping fail from some servers, they declare it dead, and remove from the cluster membership ring __eou__	user	0	0	0	0
FD	it already works pretty fine on VMs and bare metal __eou__	user	0	0	0	0
FD	I did implemented an Docker membership provider __eou__	user	0	0	0	0
FD	the problem now is that for the provider, it only knows if a node is up or down only by checking docker daemon if it is up or down __eou__	user	0	0	0	0
FD	and that is our problem __eou__	user	0	0	0	0
FQ	a container can be app, but some internal thread in the application process for whatever reason is blocked __eou__	user	0	0	0	0
FD	so no new requests should go to that node __eou__	user	0	0	0	0
FD	so the idea is to ask docker (or swarm) to kill that node __eou__	user	0	0	0	0
FD	but to be sure of that, I need to detect this stallness correctly __eou__	user	0	0	0	0
CQ	how is the provider checking daemon status? __eou__	agent	0	0	0	0
FD	remember, i know nothing about Orleans, so my apologies for simpleton questions related to that __eou__	agent	0	0	0	0
GG	nah no worries :D __eou__	user	0	0	0	0
GG	that is another problem :D __eou__	user	0	0	0	0
IG	if you are simply doing adocker ps | grep MY_CONTAINER_NAME | grep Runningor something of the nature, it's simple to also do something like:docker exec MY_CONTAINER_NAME ps aux | grep processordocker exec MY_CONTAINER_NAME ping somehost.com __eou__	agent	0	0	0	0
CC	in linux, you map docker.socket and you make docker API requests to it __eou__	user	0	0	0	0
FD	but on windows, there is no docker.socket!!! o.o __eou__	user	0	0	0	0
IG	imo it's best to keep that check external to the container... i like to couple things downstream instead of upstream... but that's preference __eou__	agent	0	0	0	0
CC	we have this check external __eou__	user	0	0	0	0
FD	each node of Orleans cluster (which is a container) send health messages to each other __eou__	user	0	0	0	0
FD	the rendevour point of those messages, used to be the membership table __eou__	user	0	0	0	0
FD	if multiple nodes try to reach a node and fail (lets say 3 times), that node is marked as dead __eou__	user	0	0	0	0
FD	even if the machine is running __eou__	user	0	0	0	0
FD	so noone will send a message to it again __eou__	user	0	0	0	0
FD	now, in docker, we don't have that table __eou__	user	0	0	0	0
FD	we were relying on docker labels __eou__	user	0	0	0	0
FD	so each Orleans node should query docker (or swarm) daemon periodically about all the containers with a given Label that are running __eou__	user	0	0	0	0
FD	that would make them know who is on the cluster __eou__	user	0	0	0	0
FD	if a container is killed/exit, next query on docker api will return all containers - that one __eou__	user	0	0	0	0
IG	that is ok for periodic refreshes and consider only the container up or down __eou__	user	0	0	0	0
FD	the problem is internal __eou__	user	0	0	0	0
FD	I need to mark a container as dead even if it is running __eou__	user	0	0	0	0
CQ	could you then iterate over all the containers and exec a "health check" script that is consistently implemented? __eou__	agent	0	0	0	0
CQ	even if it does NOTHING? __eou__	agent	0	0	0	0
CC	the problem is not the healthcheck mecanism __eou__	user	0	0	0	0
FD	we already have it __eou__	user	0	0	0	0
FD	the problem is how to mark it as dead __eou__	user	0	0	0	0
CQ	errr, other than just killing the container? __eou__	agent	0	0	0	0
CQ	or can you modify a label? __eou__	agent	0	0	0	0
CC	ok way, let me rephrase __eou__	user	0	0	0	0
IG	i haven't messed with labels really outside of RancherOs __eou__	agent	0	0	0	0
FD	the problem is how to save the counted votes to kill that node __eou__	user	0	0	0	0
IG	so my knowledge there is low as well, other than you can kinda query against them __eou__	agent	0	0	0	0
FD	remember, before docker, we had that table as a central point __eou__	user	0	0	0	0
FD	now I have labels __eou__	user	0	0	0	0
FD	so the ideal would be __eou__	user	0	0	0	0

OQ	And how would I make that base image? __eou__	user	0	0	0	0
FD	the problem I'm having is with the devel packages that I need to install, and then remove in the runtime image __eou__	user	0	0	0	0
FD	but those packages come with some dependencies __eou__	user	0	0	0	0
CQ	are you building a Windows NanoServer image or something on an ubuntu/CentOS base? __eou__	agent	0	0	0	0
FD	and I'm not sure on what's the best way to keep track of those __eou__	user	0	0	0	0
CC	I'm basing off the python image.. so eventualy it's debian __eou__	user	0	0	0	0
CQ	so the runtime image uses a package that you are compiling in the devel image or it requires an environment similar to the devel image minus a few devel packages? __eou__	agent	0	0	0	0
CC	I need gcc to compile some python modules __eou__	user	0	0	0	0
FD	and I don't need gcc in the runtime image __eou__	user	0	0	0	0
FD	for example __eou__	user	0	0	0	0
FD	gcc is not the only one __eou__	user	0	0	0	0

OQ	i host all the websites __eou__	user	0	1	1	2
FD	 [<-CODE->]  __eou__	user	0	1	0	0
FD	i think this will just run very good __eou__	user	0	1	0	0
FD	i can add a volume to my wordpress to load my wordpress files from my host volume where i save all my stuff.. how to do the volume to it? __eou__	user	0	1	0	0
CC	dragon788: you know what i mean? __eou__	user	0	1	0	0
IG	 [<-CODE->]  [<-CODE->] iirc __eou__	agent	1	1	0	1

OQ	Hello all! Maybe you guys can help me out, been beating this up all day. __eou__	user	0	0	0	0
FD	I'm trying to create a base docker image that would allow me to use volumes and have them be created with the same permission as the host. This is the base image for our containers: __eou__	user	0	0	0	0
FD	 [<-LINK->] __eou__	user	0	0	0	0
FD	This is the Dockerfile for using it in an application: __eou__	user	0	0	0	0
FD	 [<-LINK->] __eou__	user	0	0	0	0
IG	FuzzOli87: you can see if this gives you any more clarity [<-LINK->] __eou__	agent	0	0	0	0

OQ	can i use docker in a virtualbox ? __eou__	user	0	1	1	2
PF	yes __eou__	agent	1	1	0	1
FD	just be careful with docker and vagrant __eou__	agent	1	1	0	1
FD	docker inside a vm through vagrant gets effed __eou__	agent	1	1	0	1
FQ	some link about this ? __eou__	user	0	1	0	0
CQ	about vagrant? __eou__	agent	0	1	0	0
CQ	Or docker in a vm? __eou__	agent	0	1	0	0
CC	about docker run in vbox __eou__	user	0	1	0	0
IG	just install and use normally __eou__	agent	0	1	0	0
FD	docker can basically inside anything with a linux kernel __eou__	agent	0	1	0	0
FD	docker on windows used to be docker through a linux vm __eou__	agent	0	1	0	0
FD	docker on amazon ecs is docker running on a vm __eou__	agent	0	1	0	0
FQ	yes, i install in my windows but the problem is activate hyper and break the virtualbox __eou__	user	0	1	0	0
GG	haha __eou__	agent	0	1	0	0
PF	yes __eou__	agent	0	1	0	0
IG	a host operating system can only have 1 hypervisor __eou__	agent	1	1	0	1
FQ	i write a command that i can select in the booting if activate/desactivate hyper __eou__	user	0	1	0	0
FD	but the docker not run if i need vbox __eou__	user	0	1	0	0
FD	and vbox no run if i select hyper and no like this __eou__	user	0	1	0	0
FD	i need can use the two without reboot __eou__	user	0	1	0	0
IG	on windows, you cannot run both hyper and vbox __eou__	agent	0	1	0	0
FD	on any os __eou__	agent	0	1	0	0
AC	you cant run competing hypervisors __eou__	agent	1	1	0	1
GG	to me knowledge at least __eou__	agent	0	1	0	0
AC	if you have hyperv just use that it can do everything vbox does __eou__	agent	1	1	0	1

OQ	Hi all ... I starting my container via docker-compose with this command:/bin/bash -c "envsubst \'$$NGINX_UPSTREAM\' < /etc/nginx/nginx.conf.template > /etc/nginx/nginx.conf && nginx -g \'daemon off;\'" __eou__	user	0	1	1	2
FD	all is fine... but docker kill -s HUP <containerID> does not work :( __eou__	user	0	1	0	0
FD	docker exec <containerID> nginx -s reloadworks fine ...  __eou__	user	0	1	0	0
IG	scippio: most probably because you're not using a signal forwarding process manager like https://github.com/krallin/tini __eou__	agent	1	1	0	1
UF	marcelmfs: ok... thanks.. I'll try tini.. __eou__	user	0	1	0	0
CC	marcelmfs: is this right? [<-CODE->]  __eou__	user	0	1	0	0
UF	scippio: I'm not sure.  __eou__	agent	0	1	0	0
IG	In order for tini to work as expected, it needs to be your entrypoint, and be assigned PID 1. __eou__	agent	1	1	0	1
FD	only then tini will forward signals to it's child processes __eou__	agent	0	1	0	0
PF	marcelmfs: I had resolved the issue ...  __eou__	user	0	1	0	0
AC	I created a bash scrip with all my commands + nginx exec at the end and tini as entrypoint ... and all is fine now :) __eou__	user	1	1	0	1

OQ	Hi, I have a volume mapped from host to container in docker-compose.yml but the container files don’t seem to update when I change the host file. Any suggestions? __eou__	user	0	1	1	2
CQ	arjunurs: the volume is mapped in Dockerfile? __eou__	agent	0	1	0	0
CC	darkSasori: the volume is mapped in docker-compose.yml file [<-CODE->] __eou__	user	0	1	0	0
FD	darkSasori: I have this in the Dockerfile [<-CODE->] __eou__	user	0	1	0	0
IG	arjunurs: and in your Dockerfile has the declaration of volume```VOLUME /source __eou__	agent	1	1	0	1
FD	arjunurs: you need addVOLUME /sourceto work __eou__	agent	1	1	0	1

OQ	also I don't understand how distributing images works. Ithinkwhat happens is that you 'push' an image to a repo, which is actually sending meta data + a bunch of layersafterchecking the repo to see if it already has those layers or not. As in it only sends up what it needs to __eou__	user	0	1	1	2
FD	is that correct? __eou__	user	0	1	0	0
FD	well the scary part was the hacks I saw to get specific layers not to cache __eou__	user	0	1	0	0
FD	which I undrestand busts everything after __eou__	user	0	1	0	0
IG	siassaj: you are correct, if you are building on top of the same base layer for a specific image "repository" then it will already have that base layer if you have previous pushed an image to it, then it only has to transfer the newer/different layers __eou__	agent	1	1	0	1
CQ	what tricks for cache busting are you referring to? __eou__	agent	0	1	0	0
CC	dragon788: i saw a few, i don't remember exactly off the top of my head. __eou__	user	0	1	0	0
FD	so i need to create an image to serve as my 'base' image for my application images __eou__	user	0	1	0	0
FD	so instead of FROM ruby:2.3.1 I'd rather do FROM my-company-app-env __eou__	user	0	1	0	0
FD	and I suspect that I'll have a bunch of other utility apps with their own docker images, possibly basing off other app enviroments, such as my slackbot etc __eou__	user	0	1	0	0
FD	these 'base' images that I want to use, should each have a director and a Dockefile __eou__	user	0	1	0	0
FD	or should I just create a dir like 'company-dockerfiles',  and put 'app-env.dockerfile', 'slackbot-env.dockerfile', etc __eou__	user	0	1	0	0
FD	? __eou__	user	0	1	0	0

OQ	Is it possible to deploy code on docker swarm using a dockerfile added into the code __eou__	user	0	0	0	0
FD	? __eou__	user	0	0	0	0
CQ	munish-dhiman: can you clarify what you’re trying to accomplish? __eou__	agent	0	0	0	0
FD	(btw there’s a docker/swarm chat too :) ) __eou__	agent	0	0	0	0

OQ	can docker-machine be scripted? I'd like to have a script to setup my env __eou__	user	0	1	1	2
FD	right now I'm having little luck with it __eou__	user	0	1	0	0
IG	you can write a shell script which uses docker-machine __eou__	agent	1	1	0	1

OQ	Hi. I want to know if I can run multiple websites on a server, served by multiple containers. __eou__	user	0	1	1	2
IG	KRIOFT: Yes; use something like ELB to do the port mappings then run containers on separate ports __eou__	agent	1	1	0	1
UF	I understand, I will try. Thanks! __eou__	user	0	1	0	0

OQ	how do I get the output of that command into a production enviroment.... and if my DEV compose uses a container for DB.... then what  should PROD use. (considering alot of people seem to think docker+db+prod === bad) __eou__	user	0	1	1	2
AC	iDVB: i PM'd you __eou__	agent	0	1	0	0
IG	iDVB: may check hackmd, we build an awesome docker container there __eou__	agent	1	1	0	1

OQ	Sorry if this is the wrong channel, but can anyone tell me where I might see the log output for an AWS ECS container I'm trying to start a docker container on? I think it's failing, but I have no idea where to see if it is. __eou__	user	0	1	1	2
IG	Might be a better place, but I'm having issues getting php-fpm and nginx working (in separate containers),  __eou__	agent	1	1	0	1
AE	I'm getting a bad gateway and I think it might be because nginx can't communicate with php properly, but I'm not sure why __eou__	agent	1	1	0	1

OQ	guys, the container id by default is always be set as hostname, right? __eou__	user	0	1	1	2
IG	@quaddo:matrix.org from what I'm seeing, the container id is always a uniquely generated hexadecimal number __eou__	agent	1	1	0	1
IG	the container ID is a random GUID, you can pass the--name somenameto adocker runcommand to give it a name you can reference, but it needs to be unique __eou__	agent	1	1	0	1
GG	galvesribeiro: ^^ __eou__	agent	0	1	0	0
GG	hey __eou__	user	0	1	0	0
JK	hum... __eou__	user	0	1	0	0
FQ	the point is not the container name actually __eou__	user	0	1	0	0
FD	I'm trying to get the hostname itself __eou__	user	0	1	0	0
IG	@quaddo:matrix.org ah __eou__	agent	0	1	0	0
FD	@quaddo:matrix.org hostname is the same as the container id __eou__	agent	0	1	0	0
CC	unless you specify the --name __eou__	user	0	1	0	0
CQ	@quaddo:matrix.org why would you need to change that? __eou__	agent	0	1	0	0
CQ	the hostname from outside of the container or inside? __eou__	agent	0	1	0	0
CC	I don't need to change __eou__	user	0	1	0	0

GG	ave __eou__	user	0	1	1	2
OQ	I am curious, when I want to deploy an image that I've built locally is it possible (or even plausible) to upload only that top application layer that's changed, and let my machines fetch the lower layers from cache? __eou__	user	0	1	0	0
AE	siassaj: I think the layers are introduced for reducing the payload during the uploads. So, the layers are automatically recognized and only those layers that are changed, will be sent as payload. __eou__	agent	1	1	0	1
PF	Lakshman-LD: ok, that makes sense.  __eou__	user	0	1	0	0
AE	So i think maybe I need a server on the same network/data centre as my app servers that will pull from github, build the image then deploy (meaning only the shared layers get sent to the app servers) __eou__	user	1	1	0	1
AC	so probably time to go figure out how to do that,  __eou__	user	0	1	0	0
UF	thank you :) __eou__	user	0	1	0	0
GG	:) __eou__	agent	0	1	0	0
IG	siassaj: there is a docker container for the registry that is free to use __eou__	agent	1	1	0	1

OQ	Hello, I'm new on docker, I would like to know if there is some UI and/or orchestrator that allow me to manage multiple cluster in one place. I understand that there is many Docker UI available but it seems that it allows to manage only one cluster (or maybe I miss something). Indeed my cluster will be hosted on Azure, in different Vnet and I would like a single interface to manage them all. Do you have any idea? __eou__	user	0	1	1	2
IG	check rancher.com __eou__	agent	1	1	0	1
UF	interesting thanks@marcelmfs! __eou__	user	0	1	0	0

OQ	and a docker image isn't 1 file is it, it's a bunch of slices and some meta data describing which slice in which order, eh? __eou__	user	0	0	0	0

GG	hi all __eou__	user	0	0	0	0
OQ	Is there a way to set folder ownership on mounted volume? __eou__	user	0	0	0	0
FD	Somehow, when I change permissions once, it’s kept on my local docker but not on AWS. __eou__	user	0	0	0	0
GG	ShurikAg: I have a really similar file permission issue. So upvote for any helpful tipp on this :) __eou__	agent	0	0	0	0

OQ	hey everyone, are there any good patterns for copying production credential files to an image? We were thinking of putting our credential files on S3 and downloading them __eou__	user	0	1	1	2
IG	aaronmcadam:  [<-LINK->]  __eou__	agent	1	1	0	1
FD	aaronmcadam:  [<-LINK->]  __eou__	agent	1	1	0	1
UF	Thanks@Speechkey_twitter,  __eou__	user	0	1	0	0
FQ	does that support using files? __eou__	user	0	1	0	0

OQ	hey guys, so I am wondering how you all manage your docker compose definitions, say I have an api, built in a microservice architecture, it results in something like 50+ docker compose services and I am wondering, if there is a better way to setup all these services using tools instead of hardcoded yml files on disk __eou__	user	0	1	1	2
FD	I guess a nicer way would be through some interface, where I can build the configurations and house them in a tool which I could run, store them in a backend database, then I could manage them easier, any tools which you guys run that would help? or do you run everything using docker run? I guess not, cause multi service architectures would be a bit tricky to manage __eou__	user	0	1	0	0
IG	I use Rancher __eou__	agent	1	1	0	1
PF	ok, thats a good answer, thanks __eou__	user	0	1	0	0

GG	hi __eou__	user	0	1	1	2
OQ	i would like to dockerize a project that consist of:a frontend codebase (angular 2, after npm run build all the STATIC files i need is in the dist directory)\na backend codebase (a nodejs app w/o any transpilation build process)the production nginx.conf file looks like this: [<-CODE->] the backend api got a dependency for mongodb, so i will need a node, mongo, nginx Dockerfile __eou__	user	0	1	0	0
FD	what i am not sure is,  to which image's filesystem should i put the frontend static files? It should be in the nginx image. Am i correct? __eou__	user	0	1	0	0
IG	if it 100% static its better to put them into ngixn, but you can mount physical directory into both images, its depends on how you will build your containers __eou__	agent	1	1	0	1
IG	If they're only needed in the frontend I'd put them there, yes. After you all you don't ship a nginx but your frontend. __eou__	agent	1	1	0	1

OQ	dragon788: docker 1.10 at least and supports compose and doesn't do weird shit like codefresh __eou__	user	0	1	1	2
IG	tried AWS or looked at Linode and managing your own version of Docker? __eou__	agent	1	1	0	1

OQ	Hello, I want to be able to clone a repo into my docker container at run time. Because I'm using SSH to get read-only access to my repo I need to have the keys within the docker container somewhere. Is it safe to keep the public and private keys within the containers files and then link the volume in the docker-compose file? or should I be doing this another way? __eou__	user	0	1	1	2
IG	jjohnson1994: it depends on your "secretness". If it\'s a deployment key and it\'s not a problem if everyone has it. Include it. If it is a problem, you can thing aboutmount the key to container\nadd it as environment variables\nwait vor 1.13 and add it as secret __eou__	agent	1	1	0	1
PF	SISheogorath: It's just a deployment key so should be okay, thanks __eou__	user	0	1	0	0
IG	You can think about it like about "would I include that into  my github repository". __eou__	agent	1	1	0	1

OQ	I think, I have something important to say.It's the second time that this happens with me. I install docker on Linux Mint and after some hours the disk is full. Dont know why this happens, but I found out one thing, that also happens these two times. I lost all my system backups (I use timeshift). I dont know if it's some kind of conflit, but I think I have to report this, because it's a shame. The last time I had to reinstall my system, because I couldnt even start the system. This time, when I realise I had just 6GB left, I deleted some stuff here to have at least 20GB free, then disconnect from internet, restart, delete all docker images (Yes, I installed yesterday and I downloaded 4 images) then uninstall docker, and the magic happens. 100GB free right now ... Someone tells me. How this is even possibel that docker uses 100GB ? I was watching the free space decreasing. C'mon. Fix that. Until there, I will be using docker on a virtual machine, because I cant trust this software anymore. __eou__	user	0	0	0	0
JK	"Fix that." __eou__	agent	0	0	0	0
PF	great feedback __eou__	agent	0	0	0	0

OQ	Hello, I'm trying to install mysql-server in my Dockerfile but the script breaks when the MySQL installer asks to set the root password. Is there a way I can supply a password in the Dockerfile somewhere? I'll put my Dockerfile bellow... [<-CODE->]  __eou__	user	0	1	1	2
CQ	why are you making a mysql image instead of using the official one? __eou__	agent	0	1	0	0
CC	I'm not making a MySQL images, plus I just want to learn Docker __eou__	user	0	1	0	0
IG	you're making an image with mysql in it __eou__	agent	1	1	0	1
GG	:P __eou__	agent	0	1	0	0
FQ	jjohnson1994: have you tried installing quietly using the -q -y switch __eou__	agent	0	1	0	0
FD	FROM ubuntu:16.04RUN apt-get update && apt-get install -q -y \\git \\nginx \\mysql-server \\php7.0-fpm php-mysql \\php7.0-mysql __eou__	agent	0	1	0	0
IG	the official mysql image gets the root password from an environment variable you supply when you run it in a container __eou__	agent	0	1	0	0
UF	rollymaduk: I'll try that now, thanks :) __eou__	user	0	1	0	0
CQ	I don't know how they implement that though __eou__	agent	0	1	0	0
IG	FYI I agree with@Forecaster.. you really should use the official image and its best practice to have a single container manage a single app/service, you can alway connect them by linking __eou__	agent	0	1	0	0
FQ	Forecaster: &@rollymadukso would you leave mysql-server out of the Dockerfile and include the image later on in the docker-compose.yml? __eou__	user	0	1	0	0
AC	I'm trying to set up a LEMP environment if it helps __eou__	user	0	1	0	0
IG	jjohnson1994: I have Nginx, PHP and MySQL all in separate containers __eou__	agent	0	1	0	0
FD	I use pre-made images for each of them __eou__	agent	0	1	0	0
FD	mysql:latestphp:7-fpmnginx:latest __eou__	agent	0	1	0	0
FD	There is also a pre-made but unoffical nginx+fpm image, but that uses php5 __eou__	agent	0	1	0	0
UF	Thanks@Forecaster,  __eou__	user	0	1	0	0
FQ	would you mind sharing your docker-compose.yml? __eou__	user	0	1	0	0
IG	 [<-CODE->]  __eou__	agent	0	1	0	0
FD	There __eou__	agent	0	1	0	0
UF	Thanks a lot@Forecaster, I'm missing a lot of that in mine __eou__	user	0	1	0	0
GG	No problem __eou__	agent	0	1	0	0

OQ	I also have 2 instructions, one updates my ruby gems, and one updates my node packages. No matter how i go about this, at some point one of those cache layers will be invalidated for no reason because the other changes __eou__	user	0	1	1	2
FD	the Dockerfile is a naively simple tool __eou__	user	0	1	0	0
FD	would have been much more powerful if they had given us a ruby DSL to build out dockerfile descriptions, that way we could have implemented caching as we pleased __eou__	user	0	1	0	0
FD	bbl __eou__	user	0	1	0	0
AE	hmm, wish I could help, but I'm equally as stuck __eou__	agent	0	1	0	0
IG	siassaj: you simply combine your two commands into one "call" and this would create a single layer with all the changes from both, IIRC you can also while building a container run a couple commands from the command line and then "commit" afterwards in order to control when the layer actually gets saved __eou__	agent	1	1	0	1

OQ	I've pretty well got dev docker workflow working. I'm starting to loose hope for that to translate into a prod docker stack __eou__	user	0	1	1	2
FD	docker-compose.ymlseems like a great implementation/standard file.... why did we feel the need to createdocker-cloud.yml? __eou__	user	0	1	0	0
IG	iDVB: this is called "Docker ecosystem around Docker" __eou__	agent	1	1	0	1
FD	Anyway __eou__	agent	1	1	0	1
FQ	galvesribeiro: Sorry, just burning out trying to ramp up to all things Docker/Containerizing. Still have a ton to learn about related best practises etc. Not wanting to deal with the Ops side of things. Docker Cloud seems to promise as much.... but requires another filedocker-cloud.ymlcreated, so I'm curious why. docker-compose seems to contain much of the same. __eou__	user	0	1	0	0
FD	The same can be said about many other docker hosts or CI tools. Eg. my examples of codeship and cloud66 __eou__	user	0	1	0	0
IG	iDVB: it\'s really annoying yes :D but everyone have their own "best way to do it" so mhm have to deal with it __eou__	agent	0	1	0	0
UF	SISheogorath: thanks. I'm starting to see that now.  __eou__	user	0	1	0	0
AE	I guess I'm a perfectionist and struggling with needing to guess at the best foot forward.  Wish someone would feel opinionated enough to just point to a workflow that seems to work well for them.   __eou__	user	0	1	0	0
FQ	Docker Cloud looks decent but it is prod ready? Cloud66 looks like ikea for docker (in a good easy way) but I'mn struggling with their build process. __eou__	user	0	1	0	0

OQ	are there any strategies to speed up docker COPY commands. I have to copy several cache directories from host into my image so that my asset compilation and app build is fast. But these directories are full of files. __eou__	user	0	1	1	2
FD	Is it faster to tar them first, transfer the tar and untar it (or use ADD) __eou__	user	0	1	0	0
FD	even ENV commands take several seconds to complete __eou__	user	0	1	0	0
FD	perhaps I'll load host volumes instead of copying __eou__	user	0	1	0	0
FD	ah i see I'd have to mount it, run my commands then commit the container to a new image __eou__	user	0	1	0	0
IG	siassaj: use volumes bind mounts. I have a developer user, which has in it's$HOME all cached folders from npm, bower, ivy2, and so on. Then I only have-v $HOME:/home/docker_userindocker run __eou__	agent	1	1	0	1

OQ	Gulp watch is not working on docker with volume sharing ,could anyone share some suggestion to make it working __eou__	user	0	1	1	2
IG	Iamgowtham29_twitter: webpack works with it: [<-LINK->]  __eou__	agent	1	1	0	1
UF	@galvesribeiro thanks for the suggestion I ll look on to that __eou__	user	0	1	0	0

OQ	Hello. Help solve the problem please.Dockerfile: [<-CODE->] When I run a container returns an error: [<-CODE->] (but /root/run.sh File exists) __eou__	user	0	1	1	2
FD	in Dockerfile all right, I'm wrong here\nNow I will try, thank you __eou__	user	0	1	0	0
FD	 [<-CODE->] __eou__	user	0	1	0	0
IG	you can try also with /bin/sh __eou__	agent	1	1	0	1
FD	krzysztof-magosa:  [<-CODE->] __eou__	user	0	1	0	0
FD	Docker version 1.12.3, build 6b644ec __eou__	user	0	1	0	0
FD	try __eou__	agent	1	1	0	1
FD	CMD [ "/bin/sh", "/root/run.sh"] __eou__	agent	1	1	0	1
FD	CMD [ "/bin/sh", "/root/run.sh"]  __eou__	user	1	1	0	1
PF	it\'s work. thanks __eou__	user	0	1	0	0
PF	;-) __eou__	agent	0	1	0	0

OQ	I getError: No such image, container or task: a9952be2c74f2c6f3a7ae9faf12dc1576367bba5197fe74c243b637289fb653b __eou__	user	0	1	1	2
FD	but an exec id is not an image, container or task __eou__	user	0	1	0	0
FD	so docker inspect, as far as I can tell, does not support quering exec ids __eou__	user	0	1	0	0
AE	mikewrighton: I think you are correct, inspect is mostly for containers and images __eou__	agent	1	1	0	1

OQ	heyo.. i get error 502 bad gateway using fpm and caddyserver __eou__	user	0	1	1	2
FD	but i dont get why __eou__	user	0	1	0	0
FD	Can anybody help me with this issue? __eou__	user	0	1	0	0
IG	that message means that caddyserver can't talk to your fpm server. __eou__	agent	1	1	0	1

OQ	Hm, I'm having an odd issue __eou__	user	0	1	1	2
FD	I'm trying to set up a php <-> nginx environment __eou__	user	0	1	0	0
FD	I have them in separate containers, with a link from the nginx container to the php one called "php" __eou__	user	0	1	0	0
FD	in the nginx config I have a fastcgi_pass to "php:9000", which fails with a bad gateway "connection refused" __eou__	user	0	1	0	0
FD	if I ping "php" from inside the nginx container it tells me it\'s pinging the ip 103.224.182.212 __eou__	user	0	1	0	0
FD	but the ip of the php container is 10.42.187.89 __eou__	user	0	1	0	0
FD	if I replace "php" with 10.42.187.89 in the nginx config for the F_pass it works __eou__	user	0	1	0	0
FD	why is the link pointing to the wrong ip? D: __eou__	user	0	1	0	0
FD	it's not the host ip __eou__	user	0	1	0	0
CQ	@Forecaster - are you doing the linking at runtime via docker run --link ?  __eou__	agent	0	1	0	0
IG	might be worth posting your full command and any Dockerfile/docker-composer.yml etc.I just ran docker-composer up on this: [<-CODE->] and it links through OK (assuming you set up the nginx/php confs OK) - but the php:9000 part appears to be working. [<-CODE->]  __eou__	agent	1	1	0	1
FD	@Forecaster - also, I think --link is deprecated?https://docs.docker.com/engine/userguide/networking/default_network/dockerlinks/ __eou__	agent	1	1	0	1

OQ	Hello, is it possible to mount the mysql socket file directly into a docker container? __eou__	user	0	0	0	0
FD	Or basically any method that would allow me to connect to a vanilla mysql server running on the host. __eou__	user	0	0	0	0

OQ	i managed to get the container running. but i ended up with this problem. any ideas? __eou__	user	0	0	0	0
FD	 [<-CODE->] __eou__	user	0	0	0	0
FD	this is my docker-compose filedockerC1 __eou__	user	0	0	0	0
FD	 [<-CODE->] __eou__	user	0	0	0	0
FD	im using digitalocean $  20 /mo$0.030 /hour2 GB / 2 CPUs40 GB SSD disk3 TB transfer __eou__	user	0	0	0	0

OQ	Hey anyone using microsoft/mssql-server-Linux image? __eou__	user	0	0	0	0

OQ	hello. i use rancher and using caddyserver as container and cant forward to my wordpress container, i tried to ping the wordpress container and i gives answer, but still cant proxy to my wordpress.. you could help me figure out my problem? __eou__	user	0	1	1	2
JK	hmm __eou__	user	0	1	0	0
FD	no one can help me out? __eou__	user	0	1	0	0
IG	hi@renegoretzka you might need to ping the Rancher channel as I'm not that familiar with how that platform interacts with containers and forwarding __eou__	agent	0	1	0	0
FD	at first blush I\'d check and make sure the containers are on the same network "segment" and then that you exposed the appropriate ports from the wordpress container __eou__	agent	0	1	0	0
CC	gitter dont have a rancher channel __eou__	user	0	1	0	0
FD	i linked both, wordpress and caddy server.. and i even can ping them.. thats the weird path __eou__	user	0	1	0	0
CQ	you can rundocker exec <wordpress id> ping <caddyIP>@renegoretzka? __eou__	agent	0	1	0	0
PF	yea@dragon788 __eou__	user	0	1	0	0
IG	I don't know if the container could have telnet, but you could try runningtelnet <caddyIP> portinstead of  ping, does that work? __eou__	agent	0	1	0	0
JK	hmm __eou__	user	0	1	0	0
NF	telnet does not exist __eou__	user	0	1	0	0
CC	dragon788: it seems like caddyserver does not get any request from outside __eou__	user	0	1	0	0
FD	since caddyserver didnt write anything to log __eou__	user	0	1	0	0
FD	what can be the reason? __eou__	user	0	1	0	0
IG	if it doesn't have a port exposed or if the service isn't running properly inside that could be an issue __eou__	agent	1	1	0	1

OQ	I wonder if someone more knowledgeable than I might be able to speak to this issue: [<-ISSUE->] __eou__	user	0	0	0	0

OQ	Hi, am trying to debug a container that blows up starting my app and am down to looking at linux startup files that process various config files. Why would I be getting a permission denied when trying to copy the entire image from the root as follows:montreux:mlt-viz hkatz$ docker cp   d340eba864a1:/ ~/tmp/dockroot\nmkdir /Users/hkatz/tmp/dockroot/consul-template/config.d: permission denied? __eou__	user	0	0	0	0

OQ	Baffyis anyone having trouble starting docker on the latest Mint? __eou__	user	0	0	0	0
FD	Baffyi think it's a systemd issue __eou__	user	0	0	0	0

OQ	heyo =) does anyone use docker with caddy (as webserver)? __eou__	user	0	0	0	0

OQ	i am checking out the " --read-only” option ..was wondering how to set that as default in the docker DOCKER_OPTS. Did any one try it out? __eou__	user	0	0	0	0
FD	i want to start all my docker images RO by default and want to set RO as default option. __eou__	user	0	0	0	0
FD	some thing like this worksdocker run -t -i -d --read-only --tmpfs /run --tmpfs /tmp python:3 /bin/bash __eou__	user	0	0	0	0
FD	want to set it as default with DOCKER_OPTS __eou__	user	0	0	0	0

OQ	I'm new to docker and have been working at getting react-starter-kit up and running for dev and prod using docker-compose. I keep running into issues no matter how I seem to go about it.  Using straight docker I get it running but I can't seem to browse to the server thats running in the container. __eou__	user	0	0	0	0
FD	When I use docker-compose I get an error likesh: babel-node: not found which I think might be happening because for some reason the container is not in the right directory __eou__	user	0	0	0	0
FD	Anyone that might be able to lend a hand would be very appreciated __eou__	user	0	0	0	0
FD	Easier question.... for those looking to use docker for dev workflows that use hot reloading/auto reloading.... is there a best practise for node_modules folder? I mean the node_modules folder needs to be compiled on the container but conversely I need to setup a volume for the src so hot reloading works. Seems odd tonpm iinto a tmp folder in the dockerfile and then get docker-compose to copy that over into the host machine volume. Not to mention that folder being leftover after compose is shut down. __eou__	user	0	0	0	0
IG	iDVB: simply supplying the volume of the source root folder should do what you want __eou__	agent	0	0	0	0

OQ	Depending on whether I'm running on staging or on production __eou__	user	0	0	0	0
FD	I need to use a different nginx.config file __eou__	user	0	0	0	0
FD	But i have no idea how to do that. anyone know? __eou__	user	0	0	0	0

OQ	Hi guys, I need a docker compose to  use with php 7, postgresql, mongo e nodejs. Do you have some example? __eou__	user	0	0	0	0
FD	LuizTibo: : [<-LINK->] __eou__	agent	0	0	0	0

OQ	If I've got a docker container running docker (teamcity agent running builds) what is the best way to mount a volume in the 'parent' container into the 'child' container? __eou__	user	0	1	1	2
FD	The 'parent' container has the host docker socket bound (-v /var/run/docker.sock:) so when the agent (parent container) tries to run a new container and pass some -v flags; the -v flags don't get mounted correctly as far as I can tell __eou__	user	0	1	0	0
IG	presumably because it's not actually a child; but a sibling; and the directory trying to be mounted doesn't actually exist outside the 'parent' __eou__	user	1	1	0	1
IG	Trezamere: I haven\'t actually messed with volumes in that manner, but have you tried using "volumes_from" by creating a sibling of the parent container that maps in the volumes from the host you want to use and then having the spun up agents using  volumes_from? __eou__	agent	1	1	0	1

OQ	is it possible to auto pull volumes from registry while doing docker service create? do i have to manually ssh into all swarm nodes and do a pull of my volume(container) each time a new version is ready? __eou__	user	0	0	0	0

OQ	all: trying to find a simple way to tail or access stdout of a linked container running a simple service from another container (running tests) - I'd rather not modify the container running the service i.e. just use stdout - is there a good way to write stdout to a shared volume?? __eou__	user	0	0	0	0

OQ	I am writing a docker k8s lab handbook [<-LINK->] __eou__	user	0	0	0	0

OQ	Hello, I have an issue with swarm. I have a simple java application which tries to call another application. I use swarm for my deployment through a network overlay. The problem is that when I scale an app, I can validate that the hosts behind the VIP are updated but my client keeps calling the same addresses.I tried a lot of things like changing java DNS cache ttlAnyone ever had something like that ? __eou__	user	0	0	0	0

OQ	Anyone know of a good example code/repo for Docker(dev+prod)+Node App(requiring build) ? __eou__	user	0	0	0	0
IG	Unfortunately I had not seen a complete/good sample, and I'm not capable of open-sourcing mine atm, but if you need help feel free to ask... I will say this, check outdocker-composethe second you understand how to make a Dockerfile, and deep dive into networking and volumes, and you should get up to speed pretty quick __eou__	agent	0	0	0	0
AC	at least nothing that suited my needs and/or standards.... I took a TDD approach for development locally, and integration tests for containerized development __eou__	agent	0	0	0	0

GG	Afternoon!  __eou__	user	0	0	0	0
OQ	SISheogorath: the swarm init never worked :( __eou__	user	0	0	0	0
CQ	 I have no real idea why this happens. Are you know part of the docker community? __eou__	agent	0	0	0	0
CC	nop... got one email from Victor Coisne saying in other words, wait :( __eou__	user	0	0	0	0
GG	Oh okay :o that's new __eou__	agent	0	0	0	0
CC	the chat is probably too bloated __eou__	user	0	0	0	0
FD	so they are controlling access now unfortunately :( __eou__	user	0	0	0	0
FD	frustrating __eou__	user	0	0	0	0
IG	I aggree with that but I currently have no idea. Maybe you can file an issue on github __eou__	agent	0	0	0	0
NF	nah, don't want be an 4ss... let them take their time... __eou__	user	0	0	0	0
FD	regardless of swarm, I still have the Orleans <-> Docker stuff to deal with __eou__	user	0	0	0	0
FD	I read thru whole swarm docs yesterday (or Today? Who knows!) before sleep... it is very simple and has not much for my case actually __eou__	user	0	0	0	0
FD	the Orleans cluster deployment is not actually related to swarm... the only feature of swarm that is actually used but totally unrelated, is the placement strategy of containers on hosts and the desired state... thats it... load balancing etc, those things we will never use __eou__	user	0	0	0	0
FD	so a single standalone docker host or a swarm cluster makes no diff for me... what we care are the labels and find a way to proper health check the service __eou__	user	0	0	0	0

OQ	Hi , is there any documentation or checklist before moving application into production running in docker? __eou__	user	0	1	1	2
IG	quickeee: Well there are recommendations for production docker daemon setups … regarding to your application: propably the same you had before. __eou__	agent	1	1	0	1

OQ	even justdocker build .in the directory repeatedly builds the entire thing. i have nothing else running. not changing any code. any ideas where to look? __eou__	user	0	0	0	0
FD	how can I debug that WORKDIR /usr/src keeps making a different checksum? __eou__	user	0	0	0	0
FD	so another dev, same code, not having this problem... so my computer i guess __eou__	user	0	0	0	0
FD	SOMEONE_IMPORTANT: (lol), the problem was that i was on 1.13.0-rc beta. Rolling back to 1.12.5 stable fixed everything! __eou__	user	0	0	0	0

OQ	Im trying to replace my brew docker instance with a dockerized version - i can start the container - and see my data from within the container - but when i try and access it on the exposed port on os x - i get connection refused __eou__	user	0	0	0	0
FD	docker run -d --net po-net -p 5432:5432 -e POSTGRES_USER='rightisleft' -e POSTGRES_DB='mydb' --env 'PG_TRUST_LOCALNET=true' -v /usr/local/var/postgres:/var/lib/postgresql/data --name po-postgres postgres:9.5 __eou__	user	0	0	0	0
FD	database psql -U rightisleft -d mydbpsql: could not connect to server: Connection refusedIs the server running locally and acceptingconnections on Unix domain socket "/tmp/.s.PGSQL.5432"? __eou__	user	0	0	0	0
FD	docker ps -a show it active with the following ports - 0.0.0.0:5432->5432/tcp __eou__	user	0	0	0	0

OQ	I created a docker-compose.yml file here is the code `frontend:  image: ubuntu:14.04web:  build: .  command: apt-get update  command: apt-get install php7.0  command: apt-get install apache2  command: apt-get install subversion  volumes: [<-CODE->] ports: [<-CODE->] links: [<-CODE->]  [<-CODE->] Is there something wrong in my yml file? __eou__	user	0	0	0	0

OP	when I use docker cp it copies to local machine not my aws  EC2 is this normal? __eou__	user	0	1	1	2
CQ	@quaddo:matrix.org Gutemberg Ribeiro (Gitter): sorry, must have misunderstood where you were going with that. __eou__	agent	0	1	0	0
CQ	@quaddo:matrix.org Daniel Ram (Gitter): seems normal to me. What do you see when you rundocker cp --help? __eou__	agent	0	1	0	0
CC	somebody messed up my docker-compose file and now I need to copy the db exports from the container to the host to preserve them :( __eou__	user	0	1	0	0
IG	@quaddo:matrix.org can you login to the instance and rundocker cp there? __eou__	agent	1	1	0	1
PF	hmm good idea! __eou__	user	0	1	0	0
FQ	hmm once I ssh into the docker-machine it won't have reference to the docker containers? __eou__	user	0	1	0	0
AE	@quaddo:matrix.org can't say I've tried it __eou__	agent	0	1	0	0
AE	@quaddo:matrix.org so, try it :) __eou__	agent	0	1	0	0
IG	@quaddo:matrix.org you might need to become root __eou__	agent	0	1	0	0
FQ	I can ssh into the docker-machine but when I set the env it doesn't exist __eou__	user	0	1	0	0
IG	@quaddo:matrix.org getting in over my head on this one __eou__	agent	0	1	0	0
FD	@quaddo:matrix.org at a fundamental level, why are you not fond of copying to your local filesystem? __eou__	agent	0	1	0	0
CC	files are large. __eou__	user	0	1	0	0
FD	I would have to upload them to the EC2 afterwards and mongo upsert them __eou__	user	0	1	0	0
IG	@quaddo:matrix.org thought that might be the reason __eou__	agent	0	1	0	0
FD	@quaddo:matrix.org let me ponder this a sec __eou__	agent	0	1	0	0
FQ	when I do docker exec -it on the contianer, I can see the files __eou__	user	0	1	0	0
FD	but I can't move them from there __eou__	user	0	1	0	0
FD	when I do docker-machine ssh EC2, I can't locate the container to pull the files __eou__	user	0	1	0	0
FD	Like there should be a way to move the files from a container on the computer that hosts the container right? __eou__	user	0	1	0	0
IG	I guess there's no way arund it __eou__	user	0	1	0	0
FD	I'll just cp the big ass files and reupload them to the same instance, so odd we can't do this. __eou__	user	0	1	0	0
CQ	@quaddo:matrix.org wait, you said you can rundocker exec -iton the container, and you can indeed see the files you want? __eou__	agent	0	1	0	0
CC	matrixbot: yea I can get the files there, but if I try to mv them to a root folder, it won't do it. __eou__	user	0	1	0	0
FD	matrixbot: even as super user __eou__	user	0	1	0	0

OF	I've got an interesting issue... I'm running docker containers and the application is reporting significant time skew __eou__	user	0	0	0	0
FD	but when I check on both the host and the container using date and hwclock they are both identical and correct __eou__	user	0	0	0	0

OQ	I don't know all those microsoft languages tends to have ultra long names for their methods and objects __eou__	user	0	0	0	0
GG	hehehehehe __eou__	agent	0	0	0	0
FD	same applies to commandline programs :(dirvsls __eou__	user	0	0	0	0

OQ	Hi, looking to scale my existing compose file across multiple hosts using swarm. Any examples I could follow? __eou__	user	0	0	0	0

OQ	Please guide me how to start on learning basics of Docker DevOps __eou__	user	0	0	0	0

OQ	hey.. i got my caddyserver working.. i get a database error now from wordpress container __eou__	user	0	0	0	0
FD	this is the link [<-LINK->]  __eou__	user	0	0	0	0
FD	Error establishing a database connection __eou__	user	0	0	0	0
OQ	Hello,I got Caddyserver running. The Database is working correctly now and moved the domain to point to my new server. Somehow I get just a clear Website, even when I curl the webseite I get zero response.I have made a look into the Caddyserver logs and WordPress logs and I get the request, but no response. I also installed nginx as a container and served the nginx to Caddy to see if Caddy is proxying correctly. I get the welcome screen from nginx. So this is working fine, but when I again bind the WordPress container to Caddy its just nothing.Do you have a clue what can be the reason to get no response on the client but requests on the server?Thank you! __eou__	user	0	0	0	0

GG	hello __eou__	user	0	1	1	2
OQ	trying to run a container but I get that node modules are missing __eou__	user	0	1	0	0
CQ	mamartins: the container may assume you already rannpm installin the directory where the Dockerfile and packages.json is loacated? __eou__	agent	0	1	0	0
CC	I run CMD ls node_modules and modules are there! __eou__	user	0	1	0	0
FD	now it is working but I did nothing __eou__	user	0	1	0	0
FD	strange ... __eou__	user	0	1	0	0
IG	found what I add to make it work __eou__	user	0	1	0	0
FD	CMD ls node_modulesCMD npm start __eou__	user	0	1	0	0
FD	the first CMD __eou__	user	0	1	0	0
FD	i'll remove to check it problem returns __eou__	user	0	1	0	0
NF	ok error again __eou__	user	0	1	0	0
FD	[<-CODE->]  __eou__	user	0	1	0	0
IG	that sounds like a failure of node inside the container, you might ask in the nodejs/node channel about that __eou__	agent	1	1	0	1
UF	indeed __eou__	user	0	1	0	0

OQ	Question, anyone has a strategy to make Docker Builds faster, I don’t have to re-install my libraries and dependencies all the time I make tiny changes to my source code __eou__	user	0	1	1	2
CQ	damilare: MOUNT? __eou__	agent	0	1	0	0
FD	and then will be no need to use builds at all, build is environment only for dev __eou__	agent	0	1	0	0
CC	MOUNT? __eou__	user	0	1	0	0
FD	how do you mean, and dev env is actaullly my main concerned __eou__	user	0	1	0	0
IG	damilare: $ docker run -d -P \\--volume-driver=flocker \\-v my-named-volume:/webapp \\ <- this line--name web training/webapp python app.py __eou__	agent	1	1	0	1
FD	 [<-LINK->]  __eou__	agent	1	1	0	1
JK	hmmm __eou__	user	0	1	0	0
UF	thanks __eou__	user	0	1	0	0
AE	there must be a way to a container to talk to a docker host :( __eou__	agent	0	1	0	0
AC	this way, I dont have to build images when I make changes to my source code then __eou__	user	0	1	0	0
PF	makes sense thanks __eou__	user	0	1	0	0
IG	galvesribeiro:  [<-LINK->] and dont use windows ^^ __eou__	agent	0	1	0	0
FD	damilare: + __eou__	agent	0	1	0	0
FD	am0nshi: that is what I'm using __eou__	agent	0	1	0	0
FD	don't use windows is not an option __eou__	agent	0	1	0	0
IG	galvesribeiro: its called lxc containers (linux containers), in windows you loose all features they provide :) and u make from docker vagrant :) __eou__	agent	0	1	0	0
NF	not anymore __eou__	agent	0	1	0	0
NF	its is not lxc cotnainers-only anymore __eou__	agent	0	1	0	0
IG	@damilare Check some onbuild images. Like https://github.com/nodejs/docker-node/blob/master/7.4/onbuild/DockerfileThey are made to preinstall dependencies. __eou__	agent	1	1	0	1
PF	thanks@SISheogorath __eou__	user	0	1	0	0

OQ	Hi,I am new to docker and i am building lemp stack using docker every thing work as expected beside some permission issues, how to fix permissions so that php can create files/folders ( the right way).Knowing that i am using nginx:alpine, php:fpm-alpine and a custom container volume __eou__	user	0	1	1	2
IG	smahi: read back a little bit, I pasted a link regarding a good practice for changing permissions for an app __eou__	agent	0	1	0	0
NF	dragon788: thank you for the link.I did not understand your last question :can you jus docker pullthe image that you are building from to see if it is locally?sorry about that. __eou__	user	0	1	0	0
NF	smahi: sorry,  __eou__	agent	0	1	0	0
CC	was mixing up my conversations, that one was for another convo __eou__	agent	0	1	0	0
IG	kind of sad Gitter doesn't have threading like FlowDock, but that's probably the only thing I really miss, as everything else is very GitHub oriented with the pulls/commits/issues __eou__	agent	1	1	0	1
UF	dragon788: Thank you again for your kindness and help __eou__	user	0	1	0	0

OQ	Hey, is it normal behaviour that memory footprint of processes running in docker container are not visible on the host? I.e. when I run 10 containers where each consumes 400MiB of space(reported by docker stats), host machines /proc/meminfo:MemoryAvailable stays mostly the same. __eou__	user	0	1	1	2
CQ	are they the same container?  __eou__	agent	0	1	0	0
IG	the kernel is pretty smart about shared vs isolated memory and if it is the same basic runspace it could be the majority of that is being reused between the containers unless you launched with some special cgroup magic to force isolation __eou__	agent	1	1	0	1
IG	obernardovieira: it sounds like you performed adocker pull somehugerepositoryinstead of adocker pull somehugerepository:specifictagand ended up filling your disk __eou__	agent	1	1	0	1

OQ	docker-compose down && docker-compose build && docker-compose up, i used to be able to pair these and it would only rebuild images when things needed to be. my containers are now being rebuilt from scratch every time. any reason one could think? __eou__	user	0	1	1	2
FD	 [<-CODE->] used to be, that if i hadnt changed requirements.txt, it would reuse that cache. i have not changed requirements.txt and for some reason its bailing and not using the cache __eou__	user	0	1	0	0
CQ	is your source is changing between builds? or are you clearing any dangling containers at any point? __eou__	agent	0	1	0	0
CC	its rebuilding containers that i have definitely not changed. for example, im changing stuff in a django container, but theres a node container i never touch, and its going through all the npm install bs that usually only happens when the package.json is different (invalidates the build) __eou__	user	0	1	0	0
IG	looks like cache is actually breaking at step 3; which is very strange __eou__	agent	1	1	0	1
CQ	is something else changing the contents of/usr/srcfrom a shared volume? __eou__	agent	0	1	0	0

GG	hey guys __eou__	user	0	0	0	0
OQ	after latest update docker don't start anymore on windows __eou__	user	0	0	0	0
FD	 [<-CODE->]  __eou__	user	0	0	0	0
FD	when my PC restart and if I open the preferences app and click on restart I got this: __eou__	user	0	0	0	0
FD	 [<-CODE->]  __eou__	user	0	0	0	0
OQ	any idea on what that means? __eou__	user	0	0	0	0
CQ	Are you running from an administrator prompt? __eou__	agent	0	0	0	0
NF	Nop __eou__	user	0	0	0	0
FD	I just rebooted my pc __eou__	user	0	0	0	0

OP	Lol..@killerspaz, of course this is in fact, an issue . Here is a screenshot of  a trying to rundocker infoon a clean Windows 10 install of the latest Docker Toolbox  in three different terminal environments. It fails in all but the "Docker Quickstart terminal" Why?. There is an open discussion going on here ( [<-ISSUE->] ). Feel free to read through the details and ask questions on that thread ... we would love to here your feedback. __eou__	user	0	0	0	0
FD	The PR [<-ISSUE->] fixes this but it doesn't seem to have been reviewed yet by the team __eou__	user	0	0	0	0
IG	I told you, it's failing because you aren't actually starting boot2linux like the Docker Quickstart Terminal does... all it is, is a simple shell script initializing docker.... i don't use it and have never had any issues getting to docker __eou__	agent	0	0	0	0
FD	docker-machine start default && eval $(docker-machine env).... then i'm set __eou__	agent	0	0	0	0
FD	and I can't see your screenshot, it's too small on my screen and no way to expand it? __eou__	agent	0	0	0	0
FD	somombo: try that combo-command and i'm 99.999999% certain you'll be accessing Docker within seconds __eou__	agent	0	0	0	0
FD	the error isn't saying it can't find the docker binary, which that PR seems to focus on.. it's saying it can't connect to a pipe __eou__	agent	0	0	0	0
AC	here you go@killerspaz [<-LINK->]  __eou__	user	0	0	0	0
FD	you will be able to zoom in if you follow the link __eou__	user	0	0	0	0
IG	yeah that's the error when docker host isn't running __eou__	agent	0	0	0	0
CC	as you can see on the left docker was running __eou__	user	0	0	0	0
IG	Is it possible to have bridged network using docker? __eou__	agent	0	0	0	0
IG	try the command... guarantee it'll start working __eou__	agent	0	0	0	0
FD	reviewC:\\Program Files\\Docker Toolbox\\start.sh; you'll see it does very little with a ton of error checking __eou__	agent	0	0	0	0
FD	that's whatDocker Quickstart Terminalexecutes __eou__	agent	0	0	0	0
FD	in the end, it does exactly what i said to run __eou__	agent	0	0	0	0
AE	I know exactly what your docker-machine ... eval command is doing the problem is you have to re-run that command every time you close out of your terminal.. that's not how docker was meant to be.. and the PR fixes that __eou__	user	0	0	0	0
FD	evalportion sets the environment variables for that session... transiently .. once session is done you have to run that command again __eou__	user	0	0	0	0
IG	Mmmmm I\'m not sure I agree with the "not how docker was meant to be".... I WANT it to lose my env vars every time I restart my terminal, so I\'m not working on the wrong machine __eou__	agent	0	0	0	0
FD	the whole concept of docker in general is transient/ephemeral run-time.... to me this is right inline with that mentality. __eou__	agent	0	0	0	0
IG	never used docker toolbox or machine... __eou__	agent	0	0	0	0
IG	anyhow@killerspazi'm not here to debate toolbox, this is not the right place for this.. Just thought I'd shout out to anybody that's helping maintain the toolbox repo to give the Issue and PR a serious and honest look. I am not the only one who thinks there is something wrong docker-toolbox workflow. So if the outcome of giving it an honest look and discussing how to move forward is a decision that that PR is not the best way to do it and a better way is provided.. then that's great! I'm all for that. We invite you to engage on github. __eou__	user	0	0	0	0

OQ	has anyone had problems with nginx-proxy? __eou__	user	0	0	0	0

OQ	Hi folks, [<-CODE->]  [<-CODE->] On that way the logs should be written: [<-CODE->] __eou__	user	0	0	0	0

OQ	hi! does anyone happen to know about inspecting execution state in a container? when you execa command you can see the exec id in the container state, and the docker API allows you to query that state (e.g. to see whether it’s running) but I don’t see any docs for command line support __eou__	user	0	1	1	2
FD	in other words, my question is can you do this in the CLI tool: [<-LINK->] __eou__	user	0	1	0	0
CQ	mikewrighton: you mean this? [<-LINK->] __eou__	agent	0	1	0	0
FD	so I can run docker inspect to get the list of current exec ids for a container, but I can’t seem to actually inspect the execution state for those exec ids __eou__	user	0	1	0	0
FD	I tried running docker inspect<exec id> but gotError: No such image, container or task:<exec id> __eou__	user	0	1	0	0
FD	unless I’m missing something, it seems like this is not implemented for the CLI tool? __eou__	user	0	1	0	0
IG	maybe if you request the data as json from the container? __eou__	agent	1	1	0	1
CC	oh, how can I try that? __eou__	user	0	1	0	0
IG	oh, the default format is json it appears __eou__	agent	1	1	0	1
FD	if that doesn't contain it then you may be stuck with the api __eou__	agent	1	1	0	1
PF	yeah it looks like it, thanks anyway. __eou__	user	0	1	0	0

OQ	hey guys, how can my container access the host machine? I mean, which of the IPs the containers should hit to get to the host? __eou__	user	0	0	0	0
IG	galvesribeiro: Use the hostname of your machine. That's the easiest way __eou__	agent	0	0	0	0
NF	SISheogorath: it doesn't work :( __eou__	user	0	0	0	0
CQ	galvesribeiro: for what purposes you nedd host machine? __eou__	agent	0	0	0	0
CC	the container need to query the Docker Remote API of the host to check some stuff __eou__	user	0	0	0	0
IG	galvesribeiro: you can mount host docker.sock into your machine __eou__	agent	0	0	0	0
FD	and write http requests into it __eou__	agent	0	0	0	0
NF	there is no docker.sock in windows :( __eou__	user	0	0	0	0
IG	use unix 8) __eou__	agent	0	0	0	0
NF	I cant hehehe __eou__	user	0	0	0	0

AE	galvesribeiro: i hope you're gonna blog this stuff :P __eou__	user	0	0	0	0
CC	blog what? my failure getting docker to work with Orleans? :P __eou__	agent	0	0	0	0
PF	lol and your eventual success, of course! __eou__	user	0	0	0	0
GG	I hope so :D __eou__	agent	0	0	0	0

AE	c# is one of my favorite standards __eou__	user	0	0	0	0
FD	i love mono and what they've done for the language; .Net is also pretty great with some of the past few years' advancement __eou__	user	0	0	0	0
FD	their async/await stuff in 4.5+ is just mindblowing __eou__	user	0	0	0	0
PF	yup __eou__	agent	0	0	0	0
FD	Orleans is fully based on async/await and tasks in a distributed fashion __eou__	agent	0	0	0	0
IG	I have native C++ or NodeJS for that :D __eou__	agent	0	0	0	0
FD	sometimes in combination \\o/ __eou__	agent	0	0	0	0
IG	 [<-CODE->]  __eou__	agent	0	0	0	0
FD	that simple code in Orleans means that each of those actor tasks will run in a diff silo (a node/server in the cluster) and you get the result back __eou__	agent	0	0	0	0
PF	it is wonderful :D __eou__	agent	0	0	0	0
FD	my next fun project will be write Orleans fully in TypeScript for node.js :D __eou__	agent	0	0	0	0
FD	let give node community some real distributed framework :D __eou__	agent	0	0	0	0
GG	<3 TS. Ready for ES9 by now.... let's get to a real language! :P __eou__	user	0	0	0	0
PF	oh yes!  __eou__	agent	0	0	0	0
AE	but we have to mention that in general nodejs already includes a cluster framework :D so not sure how useful it is :D But I'll give it a try __eou__	agent	0	0	0	0

